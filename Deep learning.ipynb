{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep learning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP8zGzL2T3Lg+vpjKCVgmAK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CJxF3zzngITn"},"source":["# Deep learning"]},{"cell_type":"markdown","metadata":{"id":"95mIYTOee6FO"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/)"]},{"cell_type":"markdown","metadata":{"id":"ljC2o8Heg0eg"},"source":["**Tensore**\n","\n","Un tensore è un array n-dimensionale. In Python la sua rappresentazione più semplice è una lista"]},{"cell_type":"code","metadata":{"id":"yzZyzevsgGAX"},"source":["Tensor = list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VffHxwfVihoS"},"source":["**Alcune funzioni di utility per lavorare con i tensori definiti come qui sopra:**"]},{"cell_type":"markdown","metadata":{"id":"xFiIWCFAhKec"},"source":["\n","\n","*   Funzione per ottenere la forma (shape) di un tensore\n","\n"]},{"cell_type":"code","metadata":{"id":"OtmIsG_phFGb"},"source":["from typing import List\n","\n","def shape(tensor):\n","    sizes = []\n","    while isinstance(tensor, list):\n","        sizes.append(len(tensor))\n","        tensor = tensor[0]\n","    return sizes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y9c_ihL0ioiT"},"source":["\n","\n","*   Per capire se il tensore è monodimensionale\n","\n"]},{"cell_type":"code","metadata":{"id":"UEW68Ryyh5DU"},"source":["def is_1d(tensor):\n","    \"\"\"\n","    If tensor[0] is a list, it's a higher-order tensor.\n","    Otherwise, tensor is 1-dimensonal (that is, a vector).\n","    \"\"\"\n","    return not isinstance(tensor[0], list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"phal8LuLjHZy"},"source":["* Somma di tutti i valori in un tensore"]},{"cell_type":"code","metadata":{"id":"labpDtPPh7Ol"},"source":["def tensor_sum(tensor: Tensor) -> float:\n","    \"\"\"Sums up all the values in the tensor\"\"\"\n","    if is_1d(tensor):\n","        return sum(tensor)  # just a list of floats, use Python sum\n","    else:\n","        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n","                   for tensor_i in tensor)   # and sum up those results."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BR0wTbz8jUht"},"source":["* Applicazione di una funzione a tutti gli elementi di un tensore"]},{"cell_type":"code","metadata":{"id":"07yXiewkjQse"},"source":["from typing import Callable\n","\n","def tensor_apply(f: Callable[[float], float], tensor):\n","    \"\"\"Applies f elementwise\"\"\"\n","    if is_1d(tensor):\n","        return [f(x) for x in tensor]\n","    else:\n","        return [tensor_apply(f, tensor_i) for tensor_i in tensor]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MweefOpDjp9t"},"source":["* Tensore di tutti zeri con la stessa forma di quello passato in input"]},{"cell_type":"code","metadata":{"id":"oJKeKLbwjbT8"},"source":["def zeros_like(tensor):\n","    return tensor_apply(lambda _: 0.0, tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_59XcIk2kH0P"},"source":["* Applica la funzione passata in input agli elementi corrispondenti dei 2 tensori. I tensori devono avere la stessa forma."]},{"cell_type":"code","metadata":{"id":"HbdleO8qjv-m"},"source":["def tensor_combine(f: Callable[[float, float], float], t1, t2) -> Tensor:\n","    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n","    if is_1d(t1):\n","        return [f(x, y) for x, y in zip(t1, t2)]\n","    else:\n","        return [tensor_combine(f, t1_i, t2_i)\n","                for t1_i, t2_i in zip(t1, t2)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HNkYwT7skceB"},"source":["**Layer**"]},{"cell_type":"markdown","metadata":{"id":"REd-JiTklDb9"},"source":["* Per costruire reti neurali, occorre definire i layer, ovvero i vari strati che andranno a costruire la rete. Un layer deve poter applicare funzioni agli input e deve poter retropropagare i gradienti (backpropagation)."]},{"cell_type":"code","metadata":{"id":"IJzdjerckgNl"},"source":["from typing import Iterable, Tuple\n","\n","class Layer:\n","    \"\"\"\n","    Our neural networks will be composed of Layers, each of which\n","    knows how to do some computation on its inputs in the \"forward\"\n","    direction and propagate gradients in the \"backward\" direction.\n","    \"\"\"\n","    def forward(self, input):\n","        \"\"\"\n","        Note the lack of types. We're not going to be prescriptive\n","        about what kinds of inputs layers can take and what kinds\n","        of outputs they can return.\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def backward(self, gradient):\n","        \"\"\"\n","        Similarly, we're not going to be prescriptive about what the\n","        gradient looks like. It's up to you the user to make sure\n","        that you're doing things sensibly.\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def params(self) -> Iterable[Tensor]:\n","        \"\"\"\n","        Returns the parameters of this layer. The default implementation\n","        returns nothing, so that if you have a layer with no parameters\n","        you don't have to implement this.\n","        \"\"\"\n","        return ()\n","\n","    def grads(self) -> Iterable[Tensor]:\n","        \"\"\"\n","        Returns the gradients, in the same order as params()\n","        \"\"\"\n","        return ()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dvSm9TKylenD"},"source":["* Layer sigmoideo\n","\n","Layer che applica agli input la funzione sigmoidea $f(t)=1/(1+e^-t)$."]},{"cell_type":"code","metadata":{"id":"4_PLpCawmLJI"},"source":["import math\n","\n","def sigmoid(t):\n","  return 1/(1+math.exp(-t))\n","\n","class Sigmoid(Layer):\n","    def forward(self, input: Tensor):\n","        \"\"\"\n","        Apply sigmoid to each element of the input tensor,\n","        and save the results to use in backpropagation.\n","        \"\"\"\n","        self.sigmoids = tensor_apply(sigmoid, input)\n","        return self.sigmoids\n","\n","    def backward(self, gradient: Tensor):\n","        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n","                              self.sigmoids,\n","                              gradient)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uIeF9E9moTwE"},"source":["* Layer lineare\n","\n","Questo layer applica la funzione `dot(weights, inputs)` (*w_1 x i_1 + ... + w_n x i_n*). I parametri del layer sono inizializzati casualmente e verranno migliorati con la discesa del gradiente."]},{"cell_type":"markdown","metadata":{"id":"vdrq9iCZq5N1"},"source":["\n","\n","*   Tensori con parametri casuali:\n","\n"]},{"cell_type":"code","metadata":{"id":"lfel6j4gmZf5"},"source":["import random\n","from probability import inverse_normal_cdf\n","\n","def random_uniform(*dims: int) -> Tensor:\n","    if len(dims) == 1:\n","        return [random.random() for _ in range(dims[0])]\n","    else:\n","        return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n","\n","def random_normal(*dims: int,\n","                  mean: float = 0.0,\n","                  variance: float = 1.0) -> Tensor:\n","    if len(dims) == 1:\n","        return [mean + variance * inverse_normal_cdf(random.random())\n","                for _ in range(dims[0])]\n","    else:\n","        return [random_normal(*dims[1:], mean=mean, variance=variance)\n","                for _ in range(dims[0])]\n","                \n","def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n","    if init == 'normal':\n","        return random_normal(*dims)\n","    elif init == 'uniform':\n","        return random_uniform(*dims)\n","    elif init == 'xavier':\n","        variance = len(dims) / sum(dims)\n","        return random_normal(*dims, variance=variance)\n","    else:\n","        raise ValueError(f\"unknown init: {init}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xq9cq0UOqZM0"},"source":["\n","\n","*   Implementazione layer lineare:\n","\n"]},{"cell_type":"code","metadata":{"id":"t4yiIEaopjTx"},"source":["from linear_algebra import dot # w_1 x i_1 + ... + w_n x i_n\n","\n","class Linear(Layer):\n","    def __init__(self, input_dim: int, output_dim: int, init: str = 'xavier') -> None:\n","        \"\"\"\n","        A layer of output_dim neurons, each with input_dim weights\n","        (and a bias).\n","        \"\"\"\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","        # self.w[o] is the weights for the o-th neuron\n","        self.w = random_tensor(output_dim, input_dim, init=init)\n","\n","        # self.b[o] is the bias term for the o-th neuron\n","        self.b = random_tensor(output_dim, init=init)\n","\n","    def forward(self, input: Tensor) -> Tensor:\n","        # Save the input to use in the backward pass.\n","        self.input = input\n","\n","        # Return the vector of neuron outputs.\n","        return [dot(input, self.w[o]) + self.b[o]\n","                for o in range(self.output_dim)]\n","\n","    def backward(self, gradient: Tensor) -> Tensor:\n","        # Each b[o] gets added to output[o], which means\n","        # the gradient of b is the same as the output gradient.\n","        self.b_grad = gradient\n","\n","        # Each w[o][i] multiplies input[i] and gets added to output[o].\n","        # So its gradient is input[i] * gradient[o].\n","        self.w_grad = [[self.input[i] * gradient[o]\n","                        for i in range(self.input_dim)]\n","                       for o in range(self.output_dim)]\n","\n","        # Each input[i] multiplies every w[o][i] and gets added to every\n","        # output[o]. So its gradient is the sum of w[o][i] * gradient[o]\n","        # across all the outputs.\n","        return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n","                for i in range(self.input_dim)]\n","\n","    def params(self) -> Iterable[Tensor]:\n","        return [self.w, self.b]\n","\n","    def grads(self) -> Iterable[Tensor]:\n","        return [self.w_grad, self.b_grad]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fG5wyGYIq-xJ"},"source":["**Sequenza**\n","\n","Definiti i layer, definiamo una struttura che contenga una sequenza di layer, ovvero la rete neurale."]},{"cell_type":"code","metadata":{"id":"2YkZU05_rsyr"},"source":["from typing import List\n","\n","class Sequential(Layer):\n","    \"\"\"\n","    A layer consisting of a sequence of other layers.\n","    It's up to you to make sure that the output of each layer\n","    makes sense as the input to the next layer.\n","    \"\"\"\n","    def __init__(self, layers: List[Layer]) -> None:\n","        self.layers = layers\n","\n","    def forward(self, input):\n","        \"\"\"Just forward the input through the layers in order.\"\"\"\n","        for layer in self.layers:\n","            input = layer.forward(input)\n","        return input\n","\n","    def backward(self, gradient):\n","        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\"\n","        for layer in reversed(self.layers):\n","            gradient = layer.backward(gradient)\n","        return gradient\n","\n","    def params(self) -> Iterable[Tensor]:\n","        \"\"\"Just return the params from each layer.\"\"\"\n","        return (param for layer in self.layers for param in layer.params())\n","\n","    def grads(self) -> Iterable[Tensor]:\n","        \"\"\"Just return the grads from each layer.\"\"\"\n","        return (grad for layer in self.layers for grad in layer.grads())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zhM7gWriVa3A"},"source":["**Errore e ottimizzazione**\n","\n","Viene ora introdotta una classe generica *Loss* per implementare diverse astrazioni del calcolo dell'errore e del gradiente"]},{"cell_type":"code","metadata":{"id":"QbZlIys-WNdd"},"source":["class Loss:\n","    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n","        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\"\n","        raise NotImplementedError\n","\n","    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n","        \"\"\"How does the loss change as the predictions change?\"\"\"\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hXrUAFxbWUPo"},"source":["* Prima implementazione, l'errore è la somma dei quadrati degli errori"]},{"cell_type":"code","metadata":{"id":"aF29NW1AWTxA"},"source":["class SSE(Loss):\n","    \"\"\"Loss function that computes the sum of the squared errors.\"\"\"\n","    def loss(self, predicted, actual):\n","        # Compute the tensor of squared differences\n","        squared_errors = tensor_combine(\n","            lambda predicted, actual: (predicted - actual) ** 2,\n","            predicted,\n","            actual)\n","\n","        # And just add them up\n","        return tensor_sum(squared_errors)\n","\n","    def gradient(self, predicted, actual):\n","        return tensor_combine(\n","            lambda predicted, actual: 2 * (predicted - actual),\n","            predicted,\n","            actual)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWtG2c_5WvXx"},"source":["\n","\n","*   Introduzione di un ulteriore classe astratta: *Optimizer*. Gli optimizer serviranno ad aggioranre i parametri con nuovi valori, come quelli calcolati con la discesa del gradiente\n","\n"]},{"cell_type":"code","metadata":{"id":"P3QNJzMYWj2h"},"source":["class Optimizer:\n","    \"\"\"\n","    An optimizer updates the weights of a layer (in place) using information\n","    known by either the layer or the optimizer (or by both).\n","    \"\"\"\n","    def step(self, layer: Layer) -> None:\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YOVFhtoRXXA_"},"source":["\n","\n","*   La discesa del gradiente è il primo *Optimizer* implementato\n"]},{"cell_type":"code","metadata":{"id":"NibVd9nyXSmm"},"source":["class GradientDescent(Optimizer):\n","    def __init__(self, learning_rate: float = 0.1) -> None:\n","        self.lr = learning_rate\n","\n","    def step(self, layer: Layer) -> None:\n","        for param, grad in zip(layer.params(), layer.grads()):\n","            # Update param using a gradient step\n","            param[:] = tensor_combine(\n","                lambda param, grad: param - grad * self.lr,\n","                param,\n","                grad)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mg5Zh1I2Xn3j"},"source":["\n","\n","*   *Optimizer Momentum*. A differenza della discesa del gradiente, questo optimizer non usa solo l'ultimo step del gradiente, ma mantiene una media dei gradienti precedenti che viene aggiornata ad ogni iterazione col nuovo step.\n"]},{"cell_type":"code","metadata":{"id":"VCiAyidnX56D"},"source":["class Momentum(Optimizer):\n","    def __init__(self,\n","                 learning_rate: float,\n","                 momentum: float = 0.9) -> None:\n","        self.lr = learning_rate\n","        self.mo = momentum\n","        self.updates: List[Tensor] = []  # running average\n","\n","    def step(self, layer: Layer) -> None:\n","        # If we have no previous updates, start with all zeros.\n","        if not self.updates:\n","            self.updates = [zeros_like(grad) for grad in layer.grads()]\n","\n","        for update, param, grad in zip(self.updates,\n","                                       layer.params(),\n","                                       layer.grads()):\n","            # Apply momentum\n","            update[:] = tensor_combine(\n","                lambda u, g: self.mo * u + (1 - self.mo) * g,\n","                update,\n","                grad)\n","\n","            # Then take a gradient step\n","            param[:] = tensor_combine(\n","                lambda p, u: p - self.lr * u,\n","                param,\n","                update)"],"execution_count":null,"outputs":[]}]}