{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Decision trees.ipynb","provenance":[],"authorship_tag":"ABX9TyMhVMD2Mg07ZDX5uwKBWMEW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"46PKHiv__Za4"},"source":["# Decision trees\n","\n","Gli alberi decisionali possono essere usati come modelli di classificazione di regressione"]},{"cell_type":"markdown","metadata":{"id":"AJPP2Rdv__4z"},"source":["**Implementazione \"from scratch\" di un albero decisionale per classificazione**"]},{"cell_type":"markdown","metadata":{"id":"bGcT9U18AhCR"},"source":["\n","\n","*   Entropia\n","\n","Per costruire alberi decisionali è prima necessario definire il concetto di entropia. \n","\n","L'entropia misura il livello di incertezza legato ai dati in analisi. Preso un set di dati S, etichettati in modo da appartenere a una classe $C_1, ..., C_n$, se tutti i dati appartengono alla stessa classe ci sarà poca incertezza e dunque bassa entropia. Viceversa, se i dati sono distribuiti in tutte le classi in maniera uniforme, ci sarà molta incertezza e una entropia alta. \n","\n","Matematicamente, possiamo definite l'entropia di un dataset come:\n","\n","$H(s) = -p_1*log_2*p_1-...-p_n*log_2*p_n$\n","\n","dove $p_i$ è la percentuale di dati nella classe $i$\n"]},{"cell_type":"markdown","metadata":{"id":"jSactgXaDTXK"},"source":["\n","\n","*   Quella che segue è la entropia definita sopra implementata in Python\n","\n"]},{"cell_type":"code","metadata":{"id":"Ey5v2vQ-_WsM","executionInfo":{"status":"ok","timestamp":1618817055890,"user_tz":-120,"elapsed":2686,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}}},"source":["from typing import List\n","import math\n","\n","def entropy(class_probabilities):\n","    \"\"\"Given a list of class probabilities, compute the entropy\"\"\"\n","    return sum(-p * math.log(p, 2)\n","               for p in class_probabilities\n","               if p > 0)                     # ignore zero probabilities"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDS4iBasEJQE"},"source":["\n","\n","*   Vengono ora definite altre due funzioni: la prima calcola a partire da un dataset i cui dati consistono di una tupla(input, etichetta), le probabilità delle classi, la seconda funzione calcola l'entropia di un dataset.\n","\n"]},{"cell_type":"code","metadata":{"id":"gHXG4j95EIVH","executionInfo":{"status":"ok","timestamp":1618818276015,"user_tz":-120,"elapsed":602,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}}},"source":["from collections import Counter\n","\n","def class_probabilities(labels):\n","    total_count = len(labels)\n","    return [count / total_count\n","            for count in Counter(labels).values()]\n","\n","def data_entropy(labels):\n","    return entropy(class_probabilities(labels))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0lgmgRfVFzB3"},"source":["\n","\n","*   L'entropia di una partizione di dati\n","\n","Negli alberi decisionali, a ogni step decisionale i dati vengono partizionati in dei subset. Sarà quindi utile definire l'entropia di una partizione.\n","\n","Dato il dataset $S$, se esso viene partizionato in $S_1, ..., S_n$ con percentuali $q_1, ..., q_n$ dei dati, l'entropia è definita come:\n","\n","$H = q_1*H(S_1), + ... +, q_m*H(S_m)$"]},{"cell_type":"markdown","metadata":{"id":"STRMeieCHl4P"},"source":["L'entropia di una partizione viene implementata nel seguente modo"]},{"cell_type":"code","metadata":{"id":"Xtj-hawzFBgU","executionInfo":{"status":"ok","timestamp":1618818901531,"user_tz":-120,"elapsed":462,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}}},"source":["def partition_entropy(subsets):\n","    \"\"\"Returns the entropy from this partition of data into subsets\"\"\"\n","    total_count = sum(len(subset) for subset in subsets)\n","\n","    return sum(data_entropy(subset) * len(subset) / total_count\n","               for subset in subsets)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ki_Wo5CvIoig"},"source":["\n","\n","*   Caso studio e creazione dell'albero decisionale\n","\n","*Data science from scratch* prende in esame un dataset di candidati per l'assunzione in una azienda, l'albero decisionale dovrà essere creato (addestrato) a seconda delle caratteristiche dei candidati in relazione alla buona riuscita delle loro interviste.\n","\n"]},{"cell_type":"code","metadata":{"id":"MeI6ZMwOJJsc","executionInfo":{"status":"ok","timestamp":1618819330562,"user_tz":-120,"elapsed":660,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}}},"source":["from typing import NamedTuple, Optional\n","\n","class Candidate(NamedTuple):\n","    level: str\n","    lang: str\n","    tweets: bool\n","    phd: bool\n","    did_well: Optional[bool] = None  # allow unlabeled data\n","\n","                  #  level     lang     tweets  phd  did_well\n","inputs = [Candidate('Senior', 'Java',   False, False, False),\n","          Candidate('Senior', 'Java',   False, True,  False),\n","          Candidate('Mid',    'Python', False, False, True),\n","          Candidate('Junior', 'Python', False, False, True),\n","          Candidate('Junior', 'R',      True,  False, True),\n","          Candidate('Junior', 'R',      True,  True,  False),\n","          Candidate('Mid',    'R',      True,  True,  True),\n","          Candidate('Senior', 'Python', False, False, False),\n","          Candidate('Senior', 'R',      True,  False, True),\n","          Candidate('Junior', 'Python', True,  False, True),\n","          Candidate('Senior', 'Python', True,  True,  True),\n","          Candidate('Mid',    'Python', False, True,  True),\n","          Candidate('Mid',    'Java',   True,  False, True),\n","          Candidate('Junior', 'Python', False, True,  False)\n","         ]"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NG3Fb8HcJvQT"},"source":["\n","\n","*   Algoritmo ID3\n","\n","L'albero decisionale implementerà l'algoritmo ID3, che funziona nella seguente maniera:\n","\n","1.   Se tutti i dati hanno la stessa etichetta, viene creato un nodo foglia che predice quella etichetta e l'algoritmo termina.\n","2.   Se la lista degli attributi è vuota (ovvero, non ci sono più domande da fare), viene creato un nodo fogliache predice l'etichetta più frequente e l'algoritmo termina.\n","3. In alternativa, i dati vengono partizionati su ogni attributo.\n","4. Viene scelta la partizione con la minore entropia di partizione.\n","5. Viene aggiunto un nodo decisionale sull'attributo scelto\n","6. L'algoritmo procede ricorsivamente su ogni subset della partizione sugli attributi rimanenti\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mqqTQeAxOgwZ"},"source":["\n","\n","* Bisogna ora definire una rappresentazione dell'albero\n","\n"]},{"cell_type":"code","metadata":{"id":"7I6kJXAvLeCW","executionInfo":{"status":"ok","timestamp":1618820213859,"user_tz":-120,"elapsed":894,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}}},"source":["from typing import NamedTuple, Union, Any\n","\n","class Leaf(NamedTuple):\n","    value: Any\n","\n","class Split(NamedTuple):\n","    attribute: str\n","    subtrees: dict\n","    default_value: Any = None\n","\n","DecisionTree = Union[Leaf, Split]\n","\n","hiring_tree = Split('level', {   # First, consider \"level\".\n","    'Junior': Split('phd', {     # if level is \"Junior\", next look at \"phd\"\n","        False: Leaf(True),       #   if \"phd\" is False, predict True\n","        True: Leaf(False)        #   if \"phd\" is True, predict False\n","    }),\n","    'Mid': Leaf(True),           # if level is \"Mid\", just predict True\n","    'Senior': Split('tweets', {  # if level is \"Senior\", look at \"tweets\"\n","        False: Leaf(False),      #   if \"tweets\" is False, predict False\n","        True: Leaf(True)         #   if \"tweets\" is True, predict True\n","    })\n","})"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7a6fAkULOwNU"},"source":["\n","\n","*   Funzione per classificare un input\n","\n"]},{"cell_type":"code","metadata":{"id":"iX72TB-kLs23","executionInfo":{"status":"ok","timestamp":1618820757682,"user_tz":-120,"elapsed":463,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}}},"source":["def classify(tree, input):\n","    \"\"\"classify the input using the given decision tree\"\"\"\n","\n","    # If this is a leaf node, return its value\n","    if isinstance(tree, Leaf):\n","        return tree.value\n","\n","    # Otherwise this tree consists of an attribute to split on\n","    # and a dictionary whose keys are values of that attribute\n","    # and whose values of are subtrees to consider next\n","    subtree_key = getattr(input, tree.attribute)\n","\n","    if subtree_key not in tree.subtrees:   # If no subtree for key,\n","        return tree.default_value          # return the default value.\n","\n","    subtree = tree.subtrees[subtree_key]   # Choose the appropriate subtree\n","    return classify(subtree, input)        # and use it to classify the input."],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AXEh2HQmPP5T"},"source":["\n","\n","*   Implementazione dell'algoritmo definito sopra\n","\n"]},{"cell_type":"code","metadata":{"id":"EV1sK9jONHwA","executionInfo":{"status":"ok","timestamp":1618820761504,"user_tz":-120,"elapsed":492,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}}},"source":["def build_tree_id3(inputs, split_attributes, target_attribute):\n","    # Count target labels\n","    label_counts = Counter(getattr(input, target_attribute)\n","                           for input in inputs)\n","    most_common_label = label_counts.most_common(1)[0][0]\n","\n","    # If there's a unique label, predict it\n","    if len(label_counts) == 1:\n","        return Leaf(most_common_label)\n","\n","    # If no split attributes left, return the majority label\n","    if not split_attributes:\n","        return Leaf(most_common_label)\n","\n","    # Otherwise split by the best attribute\n","\n","    def split_entropy(attribute: str) -> float:\n","        \"\"\"Helper function for finding the best attribute\"\"\"\n","        return partition_entropy_by(inputs, attribute, target_attribute)\n","\n","    best_attribute = min(split_attributes, key=split_entropy)\n","\n","    partitions = partition_by(inputs, best_attribute)\n","    new_attributes = [a for a in split_attributes if a != best_attribute]\n","\n","    # recursively build the subtrees\n","    subtrees = {attribute_value : build_tree_id3(subset,\n","                                                 new_attributes,\n","                                                 target_attribute)\n","                for attribute_value, subset in partitions.items()}\n","\n","    return Split(best_attribute, subtrees, default_value=most_common_label)"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q--X90TPNTjI"},"source":["\n","\n","*   Test dell'albero\n","\n"]},{"cell_type":"code","metadata":{"id":"yOmrRehKNPSf","executionInfo":{"status":"ok","timestamp":1618820362427,"user_tz":-120,"elapsed":456,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}}},"source":["tree = build_tree_id3(inputs,\n","                      ['level', 'lang', 'tweets', 'phd'],\n","                      'did_well')"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MKoGsgEZNYr5"},"source":["Aspettativa: True"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BucMkrkxNXMm","executionInfo":{"status":"ok","timestamp":1618820368421,"user_tz":-120,"elapsed":483,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}},"outputId":"cab743a8-7c70-4182-ed10-ec5ef597d602"},"source":["classify(tree, Candidate(\"Junior\", \"Java\", True, False))"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"rnLwxksBNaCQ"},"source":["Aspettativa: False"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ae1huSrNXcC","executionInfo":{"status":"ok","timestamp":1618820374447,"user_tz":-120,"elapsed":590,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}},"outputId":"40c4e96c-992a-4554-cdd9-c26795f30042"},"source":["classify(tree, Candidate(\"Junior\", \"Java\", True, True))"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"SKgBgg4ZNaTQ"},"source":["Aspettativa: True"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQBIDgTgNXso","executionInfo":{"status":"ok","timestamp":1618820383863,"user_tz":-120,"elapsed":689,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}},"outputId":"fb6d15ae-ef00-4fda-b931-5ff823bad94c"},"source":["classify(tree, Candidate(\"Intern\", \"Java\", True, True))"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"X0brlJQaUiPv"},"source":["\n","\n","**Random forests**\n","\n","Un problema di questo approccio alla costruzione di un modello di classificazione, è la tendenza all'overfitting. Per rispondere a questo problema si possono utilizzare le cosiddette *random forests*, ovvero un insieme di decision trees diversi tra loro. \n","\n","L'output finale in questo caso è una combinazione degli output dei vari alberi; nel caso della classificazione ogni albero esprime un \"voto\" e l'output finale è l'elemento più votato."]},{"cell_type":"markdown","metadata":{"id":"Ribzg_u3Y2s4"},"source":["\n","\n","*   Random forests in scikit-learn\n","\n","Scikit-learn fornisce varie implementazione per i decision trees, di cui una per un classificatore basato su random forests, `RandomForestClassifier`.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VGorNg9mZA1s"},"source":["\n","\n","*   Creazione di un dataset più ampio e diversificato\n","\n"]},{"cell_type":"code","metadata":{"id":"AZK9-XYGZmLG","executionInfo":{"status":"ok","timestamp":1618823883483,"user_tz":-120,"elapsed":465,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}}},"source":["                  #  level     lang     tweets  phd  did_well\n","inputs = [Candidate('Senior', 'Java',   False, False, False),\n","          Candidate('Senior', 'Java',   False, True,  False),\n","          Candidate('Mid',    'Python', False, False, True),\n","          Candidate('Junior', 'Python', False, False, True),\n","          Candidate('Junior', 'R',      True,  False, True),\n","          Candidate('Junior', 'R',      True,  True,  False),\n","          Candidate('Mid',    'R',      True,  True,  True),\n","          Candidate('Senior', 'Python', False, False, False),\n","          Candidate('Senior', 'R',      True,  False, True),\n","          Candidate('Junior', 'Python', True,  False, True),\n","          Candidate('Senior', 'Python', True,  True,  True),\n","          Candidate('Mid',    'Python', False, True,  True),\n","          Candidate('Mid',    'Java',   True,  False, True),\n","          Candidate('Intern', 'Kotlin', False, True,  False),\n","          Candidate('Intern', 'R', False, True,  False),\n","          Candidate('Senior', 'R', True, False,  True),\n","          Candidate('Mid', 'Java', True, True,  False),\n","          Candidate('Mid', 'Kotlin', False, False,  True),\n","          Candidate('Senior', 'Java', True, True,  True),\n","          Candidate('Junior', 'Python', False, False,  True),\n","          Candidate('Junior', 'C', False, False,  True),\n","          Candidate('Senior', 'Python', False, False,  False),\n","          Candidate('Junior', 'C', True, False,  False)\n","         ]"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"VwOrEatgWDUV","executionInfo":{"status":"ok","timestamp":1618825195489,"user_tz":-120,"elapsed":649,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}}},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","pd_inputs = pd.DataFrame(inputs)\n","X = pd_inputs.iloc[:, 0:4]\n","y = pd_inputs.iloc[:, 4]\n"],"execution_count":115,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a8AoaNiIfAvw"},"source":["`RandomForestClassifier` richiede degli attributi di tipo float. Essendo alcune delle variabili del dataset di tipo stringa (categorighe), esse non potranno essere convertite in autoamtico in float. Sarà quindi necessario usare un encoder come `preprocessing.LabelEncoder()`"]},{"cell_type":"code","metadata":{"id":"0b9hhV9FfBHZ","executionInfo":{"status":"ok","timestamp":1618825197099,"user_tz":-120,"elapsed":1049,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}}},"source":["from sklearn import preprocessing\n","le = preprocessing.LabelEncoder()\n","X_encoded = X.apply(le.fit_transform)\n","\n","X_train, X_val, y_train, y_val = train_test_split(X_encoded, y, test_size = 1/3, random_state = 1)"],"execution_count":116,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G4y_9qIPaQw9"},"source":["\n","*   Test di `RandomForestClassifier`\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gs4RkDV-WTR0","executionInfo":{"status":"ok","timestamp":1618825198075,"user_tz":-120,"elapsed":572,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}},"outputId":"aa70d44b-9b5b-44e5-b3e6-e2065922a647"},"source":["from sklearn.ensemble import RandomForestClassifier\n","classifier = RandomForestClassifier(random_state = 1)\n","classifier.fit(X_train, y_train)\n","classifier.score(X_val, y_val)"],"execution_count":117,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.75"]},"metadata":{"tags":[]},"execution_count":117}]},{"cell_type":"markdown","metadata":{"id":"suVUridOgEQl"},"source":["Senza modificare gli iperparametri di default si ottiene uno score di 0.75. `n_estimators`, ovvero il numero di alberi nella foresta è di default impostato a 100. La massima profondità di un albero `max_depth` è impostata di default a 2. Modificando questi due iperparametri, ci si può aspettare uno score migliore"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NugxYI6e1Ao","executionInfo":{"status":"ok","timestamp":1618825418073,"user_tz":-120,"elapsed":15132,"user":{"displayName":"Gianluca Nediani","photoUrl":"","userId":"02937520502992768929"}},"outputId":"493ee8e7-e890-4589-9740-056a0b8b1a6f"},"source":["classifier = RandomForestClassifier(n_estimators = 1000, max_depth = 5, random_state = 1)\n","classifier.fit(X_train, y_train)\n","classifier.score(X_val, y_val)"],"execution_count":122,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.75"]},"metadata":{"tags":[]},"execution_count":122}]},{"cell_type":"markdown","metadata":{"id":"Lly-_SP7g5cM"},"source":["Pur aumentando gli iperparametri non aumenta lo score. Un possibile motivo può essere il basso numero di entry nel dataset, che rende il modello overfitting, nonostante sia una Random Forest."]}]}
