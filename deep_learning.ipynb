{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJxF3zzngITn"
      },
      "source": [
        "# Deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljC2o8Heg0eg"
      },
      "source": [
        "**Tensore**\n",
        "\n",
        "Un tensore è un array n-dimensionale. In Python la sua rappresentazione più semplice è una lista"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzZyzevsgGAX"
      },
      "source": [
        "Tensor = list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VffHxwfVihoS"
      },
      "source": [
        "**Alcune funzioni di utility per lavorare con i tensori definiti come qui sopra:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFiIWCFAhKec"
      },
      "source": [
        "\n",
        "\n",
        "*   Funzione per ottenere la forma (shape) di un tensore\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtmIsG_phFGb"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "def shape(tensor):\n",
        "    sizes = []\n",
        "    while isinstance(tensor, list):\n",
        "        sizes.append(len(tensor))\n",
        "        tensor = tensor[0]\n",
        "    return sizes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9c_ihL0ioiT"
      },
      "source": [
        "\n",
        "\n",
        "*   Per capire se il tensore è monodimensionale\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEW68Ryyh5DU"
      },
      "source": [
        "def is_1d(tensor):\n",
        "    \"\"\"\n",
        "    If tensor[0] is a list, it's a higher-order tensor.\n",
        "    Otherwise, tensor is 1-dimensonal (that is, a vector).\n",
        "    \"\"\"\n",
        "    return not isinstance(tensor[0], list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phal8LuLjHZy"
      },
      "source": [
        "* Somma di tutti i valori in un tensore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "labpDtPPh7Ol"
      },
      "source": [
        "def tensor_sum(tensor: Tensor) -> float:\n",
        "    \"\"\"Sums up all the values in the tensor\"\"\"\n",
        "    if is_1d(tensor):\n",
        "        return sum(tensor)  # just a list of floats, use Python sum\n",
        "    else:\n",
        "        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n",
        "                   for tensor_i in tensor)   # and sum up those results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR0wTbz8jUht"
      },
      "source": [
        "* Applicazione di una funzione a tutti gli elementi di un tensore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07yXiewkjQse"
      },
      "source": [
        "from typing import Callable\n",
        "\n",
        "def tensor_apply(f: Callable[[float], float], tensor):\n",
        "    \"\"\"Applies f elementwise\"\"\"\n",
        "    if is_1d(tensor):\n",
        "        return [f(x) for x in tensor]\n",
        "    else:\n",
        "        return [tensor_apply(f, tensor_i) for tensor_i in tensor]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MweefOpDjp9t"
      },
      "source": [
        "* Tensore di tutti zeri con la stessa forma di quello passato in input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJKeKLbwjbT8"
      },
      "source": [
        "def zeros_like(tensor):\n",
        "    return tensor_apply(lambda _: 0.0, tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_59XcIk2kH0P"
      },
      "source": [
        "* Applica la funzione passata in input agli elementi corrispondenti dei 2 tensori. I tensori devono avere la stessa forma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbdleO8qjv-m"
      },
      "source": [
        "def tensor_combine(f: Callable[[float, float], float], t1, t2) -> Tensor:\n",
        "    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n",
        "    if is_1d(t1):\n",
        "        return [f(x, y) for x, y in zip(t1, t2)]\n",
        "    else:\n",
        "        return [tensor_combine(f, t1_i, t2_i)\n",
        "                for t1_i, t2_i in zip(t1, t2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNkYwT7skceB"
      },
      "source": [
        "**Layer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REd-JiTklDb9"
      },
      "source": [
        "* Per costruire reti neurali, occorre definire i layer, ovvero i vari strati che andranno a costruire la rete. Un layer deve poter applicare funzioni agli input e deve poter retropropagare i gradienti (backpropagation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJzdjerckgNl"
      },
      "source": [
        "from typing import Iterable, Tuple\n",
        "\n",
        "class Layer:\n",
        "    \"\"\"\n",
        "    Our neural networks will be composed of Layers, each of which\n",
        "    knows how to do some computation on its inputs in the \"forward\"\n",
        "    direction and propagate gradients in the \"backward\" direction.\n",
        "    \"\"\"\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Note the lack of types. We're not going to be prescriptive\n",
        "        about what kinds of inputs layers can take and what kinds\n",
        "        of outputs they can return.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        \"\"\"\n",
        "        Similarly, we're not going to be prescriptive about what the\n",
        "        gradient looks like. It's up to you the user to make sure\n",
        "        that you're doing things sensibly.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def params(self) -> Iterable[Tensor]:\n",
        "        \"\"\"\n",
        "        Returns the parameters of this layer. The default implementation\n",
        "        returns nothing, so that if you have a layer with no parameters\n",
        "        you don't have to implement this.\n",
        "        \"\"\"\n",
        "        return ()\n",
        "\n",
        "    def grads(self) -> Iterable[Tensor]:\n",
        "        \"\"\"\n",
        "        Returns the gradients, in the same order as params()\n",
        "        \"\"\"\n",
        "        return ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvSm9TKylenD"
      },
      "source": [
        "* Layer sigmoideo\n",
        "\n",
        "Layer che applica agli input la funzione sigmoidea $f(t)=1/(1+e^-t)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_PLpCawmLJI"
      },
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(t):\n",
        "  return 1/(1+math.exp(-t))\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def forward(self, input: Tensor):\n",
        "        \"\"\"\n",
        "        Apply sigmoid to each element of the input tensor,\n",
        "        and save the results to use in backpropagation.\n",
        "        \"\"\"\n",
        "        self.sigmoids = tensor_apply(sigmoid, input)\n",
        "        return self.sigmoids\n",
        "\n",
        "    def backward(self, gradient: Tensor):\n",
        "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
        "                              self.sigmoids,\n",
        "                              gradient)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIeF9E9moTwE"
      },
      "source": [
        "* Layer lineare\n",
        "\n",
        "Questo layer applica la funzione `dot(weights, inputs)` (*w_1 x i_1 + ... + w_n x i_n*). I parametri del layer sono inizializzati casualmente e verranno migliorati con la discesa del gradiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdrq9iCZq5N1"
      },
      "source": [
        "\n",
        "\n",
        "*   Tensori con parametri casuali:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfel6j4gmZf5"
      },
      "source": [
        "import random\n",
        "from probability import inverse_normal_cdf\n",
        "\n",
        "def random_uniform(*dims: int) -> Tensor:\n",
        "    if len(dims) == 1:\n",
        "        return [random.random() for _ in range(dims[0])]\n",
        "    else:\n",
        "        return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n",
        "\n",
        "def random_normal(*dims: int,\n",
        "                  mean: float = 0.0,\n",
        "                  variance: float = 1.0) -> Tensor:\n",
        "    if len(dims) == 1:\n",
        "        return [mean + variance * inverse_normal_cdf(random.random())\n",
        "                for _ in range(dims[0])]\n",
        "    else:\n",
        "        return [random_normal(*dims[1:], mean=mean, variance=variance)\n",
        "                for _ in range(dims[0])]\n",
        "                \n",
        "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
        "    if init == 'normal':\n",
        "        return random_normal(*dims)\n",
        "    elif init == 'uniform':\n",
        "        return random_uniform(*dims)\n",
        "    elif init == 'xavier':\n",
        "        variance = len(dims) / sum(dims)\n",
        "        return random_normal(*dims, variance=variance)\n",
        "    else:\n",
        "        raise ValueError(f\"unknown init: {init}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq9cq0UOqZM0"
      },
      "source": [
        "\n",
        "\n",
        "*   Implementazione layer lineare:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4yiIEaopjTx"
      },
      "source": [
        "from linear_algebra import dot # w_1 x i_1 + ... + w_n x i_n\n",
        "\n",
        "class Linear(Layer):\n",
        "    def __init__(self, input_dim: int, output_dim: int, init: str = 'xavier') -> None:\n",
        "        \"\"\"\n",
        "        A layer of output_dim neurons, each with input_dim weights\n",
        "        (and a bias).\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # self.w[o] is the weights for the o-th neuron\n",
        "        self.w = random_tensor(output_dim, input_dim, init=init)\n",
        "\n",
        "        # self.b[o] is the bias term for the o-th neuron\n",
        "        self.b = random_tensor(output_dim, init=init)\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        # Save the input to use in the backward pass.\n",
        "        self.input = input\n",
        "\n",
        "        # Return the vector of neuron outputs.\n",
        "        return [dot(input, self.w[o]) + self.b[o]\n",
        "                for o in range(self.output_dim)]\n",
        "\n",
        "    def backward(self, gradient: Tensor) -> Tensor:\n",
        "        # Each b[o] gets added to output[o], which means\n",
        "        # the gradient of b is the same as the output gradient.\n",
        "        self.b_grad = gradient\n",
        "\n",
        "        # Each w[o][i] multiplies input[i] and gets added to output[o].\n",
        "        # So its gradient is input[i] * gradient[o].\n",
        "        self.w_grad = [[self.input[i] * gradient[o]\n",
        "                        for i in range(self.input_dim)]\n",
        "                       for o in range(self.output_dim)]\n",
        "\n",
        "        # Each input[i] multiplies every w[o][i] and gets added to every\n",
        "        # output[o]. So its gradient is the sum of w[o][i] * gradient[o]\n",
        "        # across all the outputs.\n",
        "        return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n",
        "                for i in range(self.input_dim)]\n",
        "\n",
        "    def params(self) -> Iterable[Tensor]:\n",
        "        return [self.w, self.b]\n",
        "\n",
        "    def grads(self) -> Iterable[Tensor]:\n",
        "        return [self.w_grad, self.b_grad]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG5wyGYIq-xJ"
      },
      "source": [
        "**Sequenza**\n",
        "\n",
        "Definiti i layer, definiamo una struttura che contenga una sequenza di layer, ovvero la rete neurale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YkZU05_rsyr"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "class Sequential(Layer):\n",
        "    \"\"\"\n",
        "    A layer consisting of a sequence of other layers.\n",
        "    It's up to you to make sure that the output of each layer\n",
        "    makes sense as the input to the next layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, layers: List[Layer]) -> None:\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Just forward the input through the layers in order.\"\"\"\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        return input\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\"\n",
        "        for layer in reversed(self.layers):\n",
        "            gradient = layer.backward(gradient)\n",
        "        return gradient\n",
        "\n",
        "    def params(self) -> Iterable[Tensor]:\n",
        "        \"\"\"Just return the params from each layer.\"\"\"\n",
        "        return (param for layer in self.layers for param in layer.params())\n",
        "\n",
        "    def grads(self) -> Iterable[Tensor]:\n",
        "        \"\"\"Just return the grads from each layer.\"\"\"\n",
        "        return (grad for layer in self.layers for grad in layer.grads())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhM7gWriVa3A"
      },
      "source": [
        "**Errore e ottimizzazione**\n",
        "\n",
        "Viene ora introdotta una classe generica *Loss* per implementare diverse astrazioni del calcolo dell'errore e del gradiente. La funzione *Loss* sarà quella da minimizzare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbZlIys-WNdd"
      },
      "source": [
        "class Loss:\n",
        "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
        "        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
        "        \"\"\"How does the loss change as the predictions change?\"\"\"\n",
        "        raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXrUAFxbWUPo"
      },
      "source": [
        "* Prima implementazione, l'errore è la somma dei quadrati degli errori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF29NW1AWTxA"
      },
      "source": [
        "class SSE(Loss):\n",
        "    \"\"\"Loss function that computes the sum of the squared errors.\"\"\"\n",
        "    def loss(self, predicted, actual):\n",
        "        # Compute the tensor of squared differences\n",
        "        squared_errors = tensor_combine(\n",
        "            lambda predicted, actual: (predicted - actual) ** 2,\n",
        "            predicted,\n",
        "            actual)\n",
        "\n",
        "        # And just add them up\n",
        "        return tensor_sum(squared_errors)\n",
        "\n",
        "    def gradient(self, predicted, actual):\n",
        "        return tensor_combine(\n",
        "            lambda predicted, actual: 2 * (predicted - actual),\n",
        "            predicted,\n",
        "            actual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWtG2c_5WvXx"
      },
      "source": [
        "\n",
        "\n",
        "*   Introduzione di un ulteriore classe astratta: *Optimizer*. Gli optimizer serviranno ad aggiornare i parametri con nuovi valori, come quelli calcolati con la discesa del gradiente\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3QNJzMYWj2h"
      },
      "source": [
        "class Optimizer:\n",
        "    \"\"\"\n",
        "    An optimizer updates the weights of a layer (in place) using information\n",
        "    known by either the layer or the optimizer (or by both).\n",
        "    \"\"\"\n",
        "    def step(self, layer: Layer) -> None:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOVFhtoRXXA_"
      },
      "source": [
        "\n",
        "\n",
        "*   La discesa del gradiente è il primo *Optimizer* implementato\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NibVd9nyXSmm"
      },
      "source": [
        "class GradientDescent(Optimizer):\n",
        "    def __init__(self, learning_rate: float = 0.1) -> None:\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def step(self, layer: Layer) -> None:\n",
        "        for param, grad in zip(layer.params(), layer.grads()):\n",
        "            # Update param using a gradient step\n",
        "            param[:] = tensor_combine(\n",
        "                lambda param, grad: param - grad * self.lr,\n",
        "                param,\n",
        "                grad)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg5Zh1I2Xn3j"
      },
      "source": [
        "\n",
        "\n",
        "*   *Optimizer Momentum*. A differenza della discesa del gradiente, questo optimizer non usa solo l'ultimo step del gradiente, ma mantiene una media dei gradienti precedenti che viene aggiornata ad ogni iterazione col nuovo step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCiAyidnX56D"
      },
      "source": [
        "class Momentum(Optimizer):\n",
        "    def __init__(self,\n",
        "                 learning_rate: float,\n",
        "                 momentum: float = 0.9) -> None:\n",
        "        self.lr = learning_rate\n",
        "        self.mo = momentum\n",
        "        self.updates: List[Tensor] = []  # running average\n",
        "\n",
        "    def step(self, layer: Layer) -> None:\n",
        "        # If we have no previous updates, start with all zeros.\n",
        "        if not self.updates:\n",
        "            self.updates = [zeros_like(grad) for grad in layer.grads()]\n",
        "\n",
        "        for update, param, grad in zip(self.updates,\n",
        "                                       layer.params(),\n",
        "                                       layer.grads()):\n",
        "            # Apply momentum\n",
        "            update[:] = tensor_combine(\n",
        "                lambda u, g: self.mo * u + (1 - self.mo) * g,\n",
        "                update,\n",
        "                grad)\n",
        "\n",
        "            # Then take a gradient step\n",
        "            param[:] = tensor_combine(\n",
        "                lambda p, u: p - self.lr * u,\n",
        "                param,\n",
        "                update)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2-_76LBsUKh"
      },
      "source": [
        "\n",
        "\n",
        "**Test di una rete neurale: XOR gate**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcWNbaxCsf8Q"
      },
      "source": [
        "* Definiti ora tutti gli elementi che vanno a comporre una rete neurale (neuroni, layer, sequenze, optimizer, funzioni d'errore), è possibile crearne una ed addestrarla per funzionare da porta XOR.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iSZCZAKtDpp"
      },
      "source": [
        "\n",
        "\n",
        "*   Dati di addestramento (training set). In X le variabili in input, in y gli output corretti:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRSBsghpsyWS"
      },
      "source": [
        "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
        "ys = [[0.], [1.], [1.], [0.]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRlITbsetdMI"
      },
      "source": [
        "* Addestramento rete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4wGoTOmtaCd",
        "outputId": "45fa863d-bf53-419e-b5d7-aa282165fb74"
      },
      "source": [
        "random.seed(0)\n",
        "    \n",
        "net = Sequential([\n",
        "      Linear(input_dim=2, output_dim=2),\n",
        "      Sigmoid(),\n",
        "      Linear(input_dim=2, output_dim=1)\n",
        "  ])    \n",
        "    \n",
        "import tqdm\n",
        "    \n",
        "optimizer = GradientDescent(learning_rate=0.1)\n",
        "loss = SSE()\n",
        "    \n",
        "with tqdm.trange(3000) as t:\n",
        "      for epoch in t:\n",
        "           epoch_loss = 0.0\n",
        "    \n",
        "      for x, y in zip(xs, ys):\n",
        "          predicted = net.forward(x)\n",
        "          epoch_loss += loss.loss(predicted, y)\n",
        "          gradient = loss.gradient(predicted, y)\n",
        "          net.backward(gradient) \n",
        "          optimizer.step(net)\n",
        "    \n",
        "t.set_description(f\"xor loss {epoch_loss:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [00:00<00:00, 936089.27it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqpEEhOCv1II"
      },
      "source": [
        "**Altre funzioni di attivazione**\n",
        "\n",
        "La sigmoidea non è l'unica funzione di attivazione che si utilizza, e spesso gliene si preferiscono altre in quanto $sigmoid(0) = 1/2$; vale a dire che se la somma degli input di un neurone è 0, la funzione fornisce in output un valore positivo. Una funzione molto utilizzata $tanh$, la tangente iperbolica\n",
        "\n",
        "*   Funzione tangente iperbolica\n",
        "\n",
        "$tanh(x)=sinh(x)/cosh(x)$\n",
        "\n",
        "$d/dx(tanh(x))=1-tanh(x)^2$\n",
        "\n",
        "Ha range -1, 1 e forma sigmoidale, ma per input 0 ha output 0.\n",
        "\n",
        "\n",
        "* Implementazione in python della funzione e implementazione di un layer che la utilizza come attivazione:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNJ12biPyahh"
      },
      "source": [
        "import math\n",
        "\n",
        "def tanh(x: float) -> float:\n",
        "    # If x is very large or very small, tanh is (essentially) 1 or -1.\n",
        "    # We check for this because e.g. math.exp(1000) raises an error.\n",
        "    if x < -100:  return -1\n",
        "    elif x > 100: return 1\n",
        "\n",
        "    em2x = math.exp(-2 * x)\n",
        "    return (1 - em2x) / (1 + em2x)\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        # Save tanh output to use in backward pass.\n",
        "        self.tanh = tensor_apply(tanh, input)\n",
        "        return self.tanh\n",
        "\n",
        "    def backward(self, gradient: Tensor) -> Tensor:\n",
        "        return tensor_combine(\n",
        "            lambda tanh, grad: (1 - tanh ** 2) * grad,\n",
        "            self.tanh,\n",
        "            gradient)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_nEaSIfzcCO"
      },
      "source": [
        "\n",
        "\n",
        "*   Rectified Linear Unit (ReLU)\n",
        "\n",
        "$ReLU(x)=x^+=max(0, x)$\n",
        "Un'altra funzione spesso utilizzata è ReLU. L'output è 0 per input negativi e l'identità per input positivi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlsteVop0G9G"
      },
      "source": [
        "class Relu(Layer):\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        self.input = input\n",
        "        return tensor_apply(lambda x: max(x, 0), input)\n",
        "\n",
        "    def backward(self, gradient: Tensor) -> Tensor:\n",
        "        return tensor_combine(lambda x, grad: grad if x > 0 else 0,\n",
        "                              self.input,\n",
        "                              gradient)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRX94Stb0xP6"
      },
      "source": [
        "**Test di rete neurale: FizzBuzz**\n",
        "\n",
        "Il problema da risolvere, chiamato *Fizzbuzz*, è il seguente:\n",
        "\n",
        " *Stampare i numeri da 1 a 100, se il numero è divisibile per 3, stampare \"fizz\" invece del numero. Se il numero è divisibile per 5, stampare \"buzz\" al posto del numero. Se il numero è divisibile per 15, stampare \"fizzbuzz\" al posto del numero.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39Bc0QET15aJ"
      },
      "source": [
        "\n",
        "\n",
        "*   Encoding dell'output in un vettore di 4 numeri binari"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHh-kP3m15MN"
      },
      "source": [
        "def fizz_buzz_encode(x):\n",
        "    if x % 15 == 0:\n",
        "        return [0, 0, 0, 1]\n",
        "    elif x % 5 == 0:\n",
        "        return [0, 0, 1, 0]\n",
        "    elif x % 3 == 0:\n",
        "        return [0, 1, 0, 0]\n",
        "    else:\n",
        "        return [1, 0, 0, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NHA-_Mq2Dh_"
      },
      "source": [
        "* Encoding da decimale a binario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5orlZFy1iIH"
      },
      "source": [
        "def binary_encode(x):\n",
        "    binary: List[float] = []\n",
        "\n",
        "    for i in range(10):\n",
        "        binary.append(x % 2)\n",
        "        x = x // 2\n",
        "\n",
        "    return binary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muiBHDUV2HtT"
      },
      "source": [
        "* Funzione che ritorna l'indice del valore più grande"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7zfEqaZ1upO"
      },
      "source": [
        "def argmax(xs: list) -> int:\n",
        "    \"\"\"Returns the index of the largest value\"\"\"\n",
        "    return max(range(len(xs)), key=lambda i: xs[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7bz3t7m2SYx"
      },
      "source": [
        "* Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdq0VlQo2TtX"
      },
      "source": [
        "xs = [binary_encode(n) for n in range(101, 1024)]\n",
        "ys = [fizz_buzz_encode(n) for n in range(101, 1024)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxkZjSjG2YKt"
      },
      "source": [
        "* Creazione rete neurale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQnVbxBm2W8a"
      },
      "source": [
        "random.seed(0)\n",
        "    \n",
        "NUM_HIDDEN = 25\n",
        "\n",
        "net = Sequential([\n",
        "   Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n",
        "   Tanh(),\n",
        "   Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform'),\n",
        "   Sigmoid()\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX_vSGMt2rij"
      },
      "source": [
        "* Funzione per valutare la precisione della rete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKEqameQ2ypW"
      },
      "source": [
        "def fizzbuzz_accuracy(low, hi, net):\n",
        "    num_correct = 0\n",
        "    for n in range(low, hi):\n",
        "        x = binary_encode(n)\n",
        "        predicted = argmax(net.forward(x))\n",
        "        actual = argmax(fizz_buzz_encode(n))\n",
        "        if predicted == actual:\n",
        "              num_correct += 1\n",
        "    \n",
        "    return num_correct / (hi - low)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p__d0v723Epm"
      },
      "source": [
        "* Training rete neurale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_88KI1WN3GIB",
        "outputId": "a736bbd8-ae65-4e3c-addf-19805d53e323"
      },
      "source": [
        "optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n",
        "loss = SSE()\n",
        "    \n",
        "with tqdm.trange(1000) as t:\n",
        "    for epoch in t:\n",
        "        epoch_loss = 0.0\n",
        "    \n",
        "        for x, y in zip(xs, ys):\n",
        "            predicted = net.forward(x)\n",
        "            epoch_loss += loss.loss(predicted, y)\n",
        "            gradient = loss.gradient(predicted, y)\n",
        "            net.backward(gradient)\n",
        "    \n",
        "            optimizer.step(net)\n",
        "    \n",
        "        accuracy = fizzbuzz_accuracy(101, 1024, net)\n",
        "        t.set_description(f\"fb loss: {epoch_loss:.2f} acc: {accuracy:.2f}\")\n",
        "    \n",
        "# Now check results on the test set\n",
        "print(\"test results\", fizzbuzz_accuracy(1, 101, net))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fb loss: 64.54 acc: 0.95: 100%|██████████| 1000/1000 [12:12<00:00,  1.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test results 0.9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv23lpV_3eZG"
      },
      "source": [
        "\n",
        "\n",
        "*   Con mille iterazioni di training, la precisione raggiunta dal modello è del 90%\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPuckdlT3qGd"
      },
      "source": [
        "**Softmax**\n",
        "\n",
        "Al posto del layer sigmoidale finale, si può usare la funziona softmax, che converte il vettore di numeri reali in un vettore di probabilità. Per farlo convertiamo tutti i numeri nel vettore x in positivo usando $e^x$, e poi dividiamo ognuno dei numeri ottenuti per la somma, ottenendo la probabilità di quell'elemento del vettore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwQ3MP9c8FhM"
      },
      "source": [
        "def softmax(tensor: Tensor) -> Tensor:\n",
        "    \"\"\"Softmax along the last dimension\"\"\"\n",
        "    if is_1d(tensor):\n",
        "        # Subtract largest value for numerical stabilitity.\n",
        "        largest = max(tensor)\n",
        "        exps = [math.exp(x - largest) for x in tensor]\n",
        "\n",
        "        sum_of_exps = sum(exps)                 # This is the total \"weight\".\n",
        "        return [exp_i / sum_of_exps             # Probability is the fraction\n",
        "                for exp_i in exps]              # of the total weight.\n",
        "    else:\n",
        "        return [softmax(tensor_i) for tensor_i in tensor]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11wpBtzt_dKI"
      },
      "source": [
        "\n",
        "**Funzione di errore entropia incrociata**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ1isvcI8d_j"
      },
      "source": [
        "class SoftmaxCrossEntropy(Loss):\n",
        "    \"\"\"\n",
        "    This is the negative-log-likelihood of the observed values, given the\n",
        "    neural net model. So if we choose weights to minimize it, our model will\n",
        "    be maximizing the likelihood of the observed data.\n",
        "    \"\"\"\n",
        "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
        "        # Apply softmax to get probabilities\n",
        "        probabilities = softmax(predicted)\n",
        "\n",
        "        # This will be log p_i for the actual class i and 0 for the other\n",
        "        # classes. We add a tiny amount to p to avoid taking log(0).\n",
        "        likelihoods = tensor_combine(lambda p, act: math.log(p + 1e-30) * act,\n",
        "                                     probabilities,\n",
        "                                     actual)\n",
        "\n",
        "        # And then we just sum up the negatives.\n",
        "        return -tensor_sum(likelihoods)\n",
        "\n",
        "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
        "        probabilities = softmax(predicted)\n",
        "\n",
        "        # Isn't this a pleasant equation?\n",
        "        return tensor_combine(lambda p, actual: p - actual,\n",
        "                              probabilities,\n",
        "                              actual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAK7j4vF8y_c"
      },
      "source": [
        "net = Sequential([\n",
        "        Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n",
        "        Tanh(),\n",
        "        Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform')\n",
        "        # No final sigmoid layer now\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm6uOjE88fGA",
        "outputId": "a055e7fe-f551-40b3-e9a7-4f800ba2b06a"
      },
      "source": [
        "optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n",
        "loss = SoftmaxCrossEntropy()\n",
        "    \n",
        "with tqdm.trange(1000) as t:\n",
        "    for epoch in t:\n",
        "        epoch_loss = 0.0\n",
        "    \n",
        "        for x, y in zip(xs, ys):\n",
        "            predicted = net.forward(x)\n",
        "            epoch_loss += loss.loss(predicted, y)\n",
        "            gradient = loss.gradient(predicted, y)\n",
        "            net.backward(gradient)\n",
        "    \n",
        "            optimizer.step(net)\n",
        "    \n",
        "        accuracy = fizzbuzz_accuracy(101, 1024, net)\n",
        "        t.set_description(f\"fb loss: {epoch_loss:.2f} acc: {accuracy:.2f}\")\n",
        "    \n",
        "# Now check results on the test set\n",
        "print(\"test results\", fizzbuzz_accuracy(1, 101, net))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fb loss: 5.03 acc: 1.00: 100%|██████████| 1000/1000 [12:09<00:00,  1.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test results 0.98\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pH5EVrtBZ3m"
      },
      "source": [
        "\n",
        "\n",
        "*   Precisione del 98%\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3kv8inp87Yd"
      },
      "source": [
        "**Dropout**\n",
        "\n",
        "I modelli di reti neurali tendono all'overfitting. Per mitigare questo problema si utilizza un metodo chiamato *dropout*. Durante il training, alcuni neuroni scelti casualmente vengono spenti per fare in modo che la rete nel suo complesso non dipenda in maniera eccessiva dai singoli neuroni.\n",
        "\n",
        "Per la fase valutazione si vogliono invece usare tutti i neuroni, quindi il layer di dropout dovrà sapere in quale fase ci si trova.\n",
        "\n",
        "Inoltre, visto che il layer di dropout fornisce in output solo una parte del suo input (a causa dei neuroni spenti), in fase di valutazione ridurre nella stessa proporzione gli output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc2_M0H687Gv"
      },
      "source": [
        "class Dropout(Layer):\n",
        "    def __init__(self, p: float) -> None:\n",
        "        self.p = p\n",
        "        self.train = True\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        if self.train:\n",
        "            # Create a mask of 0s and 1s shaped like the input\n",
        "            # using the specified probability.\n",
        "            self.mask = tensor_apply(\n",
        "                lambda _: 0 if random.random() < self.p else 1,\n",
        "                input)\n",
        "            # Multiply by the mask to dropout inputs.\n",
        "            return tensor_combine(operator.mul, input, self.mask)\n",
        "        else:\n",
        "            # During evaluation just scale down the outputs uniformly.\n",
        "            return tensor_apply(lambda x: x * (1 - self.p), input)\n",
        "\n",
        "    def backward(self, gradient: Tensor) -> Tensor:\n",
        "        if self.train:\n",
        "            # Only propagate the gradients where mask == 1\n",
        "            return tensor_combine(operator.mul, gradient, self.mask)\n",
        "        else:\n",
        "            raise RuntimeError(\"don't call backward when not in train mode\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojlPvKabsO87"
      },
      "source": [
        "# Caso studio: MNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sOI2rmOsUJu",
        "outputId": "c3d790ea-4ca0-4bc4-a178-e62b56eedbb4"
      },
      "source": [
        "!pip install mnist\n",
        "import mnist\n",
        "\n",
        "mnist.temporary_dir = lambda: \"/tmp\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mnist in /usr/local/lib/python3.7/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mnist) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KetxsVMosyqv"
      },
      "source": [
        "* MNIST è un dataset di cifre scritte a mano, l'obiettivo è costruire una rete neurale come visto in precedenza ed addestrarla a riconoscere quest cifre.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTq27sT6sU5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d436e19-acf1-4559-9d33-2d7215dd4ff5"
      },
      "source": [
        "train_images = mnist.train_images().tolist()\n",
        "shape(train_images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[60000, 28, 28]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efkNpFDetULU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9d6e13-9f13-4647-85f8-47266a3e064a"
      },
      "source": [
        "train_labels = mnist.train_labels().tolist()\n",
        "shape(train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[60000]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq-vZks2yHKn"
      },
      "source": [
        "* Il set di training scaricato è di default un array NumPy, visto che in questo caso l'approccio è \"from scratch\", viene convertito a lista con `tolist()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OL7XxzKx_0S"
      },
      "source": [
        "\n",
        "\n",
        "*   Visualizzazione di 25 delle cifre\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jopiWyHttD7m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "1e82b98f-e02a-410c-aa14-c33e210510f4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(5, 5)\n",
        "    \n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        # Plot each image in black and white and hide the axes.\n",
        "        ax[i][j].imshow(train_images[5 * i + j], cmap='Greys')\n",
        "        ax[i][j].xaxis.set_visible(False)\n",
        "        ax[i][j].yaxis.set_visible(False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xZJH9dIyDw8"
      },
      "source": [
        "* Download del validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q6YwP4gydcX"
      },
      "source": [
        "test_images = mnist.test_images().tolist()\n",
        "test_labels = mnist.test_labels().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfW_ruI8y6Of"
      },
      "source": [
        "* Il layer lineare come definito prima accetta solo input monodimensionali, quindi bisognerà ridurre la dimensionionalità delle immagini come una operazione di flattening. Ad ogni immagine, viene poi sottratto il valore medio e il tutto viene diviso per 256, per avere dei valori compresi tra 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvZNXGGrz91l"
      },
      "source": [
        "# Compute the average pixel value\n",
        "avg = tensor_sum(train_images) / 60000 / 28 / 28\n",
        "    \n",
        "# Rescale, and flatten\n",
        "train_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n",
        "                  for image in train_images]\n",
        "test_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n",
        "                  for image in test_images]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lDULhk-0Zpv"
      },
      "source": [
        "* Occorre anche una funzione di one-hot-encoding, per effettuare un encoding sulle etichette, in quanto abbiamo 10 output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92O-LCeK0enS"
      },
      "source": [
        "def one_hot_encode(i, num_labels = 10):\n",
        "    return [1.0 if j == i else 0.0 for j in range(num_labels)]\n",
        "    \n",
        "train_labels = [one_hot_encode(label) for label in train_labels]\n",
        "test_labels = [one_hot_encode(label) for label in test_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESyxPtvO1SB3"
      },
      "source": [
        "* Adesso le etichette sono in forma vettoriale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW5RiMUD1N8N",
        "outputId": "4b00313d-75cb-4a00-8375-68806fc5d3a6"
      },
      "source": [
        "train_labels[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
              " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTnEUkTi1edY"
      },
      "source": [
        "* Viene adesso definita una funzione `loop`. Questo loop verrà utilizzato per l'addestramento, qualora venga passato un optimizer, o per la validazione, in caso non venga passato un optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIkSZlmL1P43"
      },
      "source": [
        "def loop(model: Layer,\n",
        "             images: List[Tensor],\n",
        "             labels: List[Tensor],\n",
        "             loss: Loss,\n",
        "             optimizer: Optimizer = None) -> None:\n",
        "        correct = 0         # Track number of correct predictions.\n",
        "        total_loss = 0.0    # Track total loss.\n",
        "    \n",
        "        with tqdm.trange(len(images)) as t:\n",
        "            for i in t:\n",
        "                predicted = model.forward(images[i])             # Predict.\n",
        "                if argmax(predicted) == argmax(labels[i]):       # Check for\n",
        "                    correct += 1                                 # correctness.\n",
        "                total_loss += loss.loss(predicted, labels[i])    # Compute loss.\n",
        "    \n",
        "                # If we're training, backpropagate gradient and update weights.\n",
        "                if optimizer is not None:\n",
        "                    gradient = loss.gradient(predicted, labels[i])\n",
        "                    model.backward(gradient)\n",
        "                    optimizer.step(model)\n",
        "    \n",
        "                # And update our metrics in the progress bar.\n",
        "                avg_loss = total_loss / (i + 1)\n",
        "                acc = correct / (i + 1)\n",
        "                t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxfJ1E4x2Y9O"
      },
      "source": [
        "1. **Test di un modello semplice di regressione logistica**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF3MGMjW14K5"
      },
      "source": [
        "\n",
        "\n",
        "*   Definito il loop, lo si può utilizzare per addestrare un semplice modello di regressione logistica. Non è altro che un layer lineare con gli output che vengono poi passato alla funzione softmax che fornisce 10 output fra 0 e 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNBU2JF32VZJ",
        "outputId": "c0b1edc4-c4d6-4b25-cc87-965d6a4755db"
      },
      "source": [
        "# Logistic regression is just a linear layer followed by softmax\n",
        "model = Linear(784, 10)\n",
        "loss = SoftmaxCrossEntropy()\n",
        "    \n",
        "# This optimizer seems to work\n",
        "optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n",
        "    \n",
        "# Train on the training data\n",
        "loop(model, train_images, train_labels, loss, optimizer)\n",
        "    \n",
        "# Test on the test data (no optimizer means just evaluate)\n",
        "loop(model, test_images, test_labels, loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mnist loss: 0.357 acc: 0.899: 100%|██████████| 60000/60000 [16:10<00:00, 61.83it/s]\n",
            "mnist loss: 0.361 acc: 0.891: 100%|██████████| 10000/10000 [00:58<00:00, 170.79it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcO5_8OQ7BQI"
      },
      "source": [
        "* Questo modello ha una precisione dell'89% a fronte di 16 minuti impiegati per l'addestramento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec2Qxync2j_y"
      },
      "source": [
        "2. **Test di un modello con rete neurale *profonda***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Grmw2K2wH6"
      },
      "source": [
        "* Questa volta il modello utilizzato ha due hidden layer, entrambi con $tanh$ come funzione di attivazione. Si fa anche utuilizzo di dropout con probabilità 0.1 in entrambi i layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aY_kVgN3Kp4"
      },
      "source": [
        "random.seed(0)\n",
        "\n",
        "dropout1 = Dropout(0.1)\n",
        "dropout2 = Dropout(0.1)\n",
        "    \n",
        "model = Sequential([\n",
        "    Linear(784, 30),  # Hidden layer 1: size 30\n",
        "    dropout1,\n",
        "    Tanh(),\n",
        "    Linear(30, 10),   # Hidden layer 2: size 10\n",
        "    dropout2,\n",
        "    Tanh(),\n",
        "    Linear(10, 10)    # Output layer: size 10\n",
        "    ])\n",
        "    \n",
        "    \n",
        "# Training the deep model for MNIST\n",
        "    \n",
        "optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n",
        "loss = SoftmaxCrossEntropy()\n",
        "    \n",
        "# Enable dropout and train (takes > 20 minutes on my laptop!)\n",
        "dropout1.train = dropout2.train = True\n",
        "loop(model, train_images, train_labels, loss, optimizer)\n",
        "    \n",
        "# Disable dropout and evaluate\n",
        "dropout1.train = dropout2.train = False\n",
        "loop(model, test_images, test_labels, loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zktbQegc5NsS"
      },
      "source": [
        "# Test di un modello con libreria Keras\n",
        "\n",
        "Dopo aver implementato due reti neurali \"from scratch\", provo ad implementarne una con la libreria Keras per risolvere lo stesso problema delle cifre scritte a mano."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_qEkthW94ER"
      },
      "source": [
        "La rete neurale è composta da due strati *dense* da 30 e 10 neuroni con attivazione *ReLU* e layer di output con attivazione *softmax*, come funzione loss si utilizza *crossentropy*, che è simile a quella implementata precedentemente ed è indicata quando le etichette hanno una rappresentazione *one-hot*, come in questo caso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqMtOhnh5j04"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(30, activation=\"relu\", input_shape=(784,)),\n",
        "    Dense(10, activation=\"relu\"),\n",
        "    Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PluS0NkL7WkD",
        "outputId": "f6ad1352-f334-4313-cccf-ecdf5f186f93"
      },
      "source": [
        "fit_history = model.fit(train_images, train_labels, epochs=10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8455 - accuracy: 0.7234\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2136 - accuracy: 0.9377\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1645 - accuracy: 0.9504\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1360 - accuracy: 0.9601\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1188 - accuracy: 0.9647\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1070 - accuracy: 0.9675\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0985 - accuracy: 0.9690\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0916 - accuracy: 0.9709\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0844 - accuracy: 0.9739\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0827 - accuracy: 0.9745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_hsboJJ749d",
        "outputId": "cdf65041-25c5-41b2-d1bf-a6e2bb17f8c3"
      },
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 1ms/step - loss: 0.1121 - accuracy: 0.9672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.11205915361642838, 0.967199981212616]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARfIb-4I-4Po"
      },
      "source": [
        "* La rete neurale creata con libreria Keras ha una precisione del 97% e ha richiesto meno di 3 minuti per l'addestramento sullo stesso dataset: un risultato nettamente migliore rispetto alle reti neurali *from scratch* implementate sopra.\n"
      ]
    }
  ]
}