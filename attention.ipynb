{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX9JbGoshFn-"
      },
      "source": [
        "# Attention e modello Transformer, capitolo 16 di *Hands-On Machine Learning... (2019)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGwM7xyVgsGN"
      },
      "source": [
        "**Elementi trattati in capitoli precedenti nel libro e citati o utilizzati nel capitlolo 16:**\n",
        "\n",
        "* Chapter 4, dot product\n",
        "* Chapter 4, gradient descent\n",
        "* Chapter 13, custom preprocessing layer\n",
        "* Chapter 13, categorical input features\n",
        "* Chapter 13, TensorFlow datasets\n",
        "* Chapter 13, look-up table\n",
        "* Chapter 13, TF Transform\n",
        "* Chapter 13, Embedding layer\n",
        "* Chapter 15, RNNs\n",
        "* Chapter 15, time-distributed Dense layer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T2gcyYHjsTK"
      },
      "source": [
        "**Esercizi di fine capitolo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg8xuyJZjzqs"
      },
      "source": [
        "1. Quali sono i pro e i contro di utilizzare una RNN *stateful* rispetto a una RNN *stateless*?\n",
        "  \n",
        "  Un modello stateless a ogni iterazione parte con uno stato *azzerato* che viene aggiornato ad ogni step. All'ultimo step, lo stato non viene conservato perchè ha terminato il suo utilizzo, e alla successiva iterazione si riparte da zero.\n",
        "\n",
        "  In un modello stateful invece lo stato all'ultimo step di una iterazione viene conservato come punto di partenza per l'iterazione successiva. Un modello stateful può quindi valutare pattern più lunghi; tuttavia un modello di questo tipo richiede più lavoro di pre-processing in quanto ogni sequenza in un batch deve cominiciare esattamente da dove finisce la precedente.\n",
        "\n",
        "2. Perchè per l'*automatic translation* si utilizzano RNN di tipo Encoder-Decoder al posto di semplici RNN *sequence-to-sequence*\n",
        "\n",
        "  Vengono preferiti modelli Encoder-Decoder in quanto possono valutare una frase nella sua interezza. I modelli sequence-to-sequence invece traducono parola per parola, e questo tipo di approccio porta a scarsi risultati.\n",
        "\n",
        "3. In che modo si possono gestire sequenze di input di lunghezza variabile? E di output?\n",
        "\n",
        "  Per quanto riguarda le sequenze di input, si possono usare dei token di padding, a patto che venga istruito un layer di masking per far sì che la rete neurale li ignori nel calcolo dell'errore. In output invece si può usare un token di *end-of-sequence*; anche in questo caso bisogna istruire la rete perchè ignori gli elementi della sequenza che vengono dopo tale token.\n",
        "\n",
        "4. Cos'è la *beam search*, e quando si utilizza? Con quale strumento si può implementare?\n",
        "\n",
        "  Beam search è una soluzione per fare in modo che il modello possa ad ogni step tenere conto delle $k$ sequenze che hanno maggiore probabilità di essere corrette. Ad ogni step ogni sequenza si allunga di una parola, e alla fine viene selezionata la sequenza, vale a dire la frase, con la maggiore probabilità.\n",
        "\n",
        "5. Cos'è un meccanismo di attenzione? Qual è la sua utilità?\n",
        "\n",
        "  Si tratta di un meccanismo che permette al modello di concentrarsi a ogni step su un particolare elemento della sequenza. Per esempio, prendendo in analisi un traduttore inglese-francese, allo step temporale in cui il decoder deve produrre in output la parola *lait*, concentrerà la sua attenzione sulla parola *milk* in input. Questo permette di abbreviare notevolmente il percorso di ogni parola in input fino alla sua traduzione, migliorando le prestazioni delle RNN per il natural language processing.\n",
        "\n",
        "6. Qual è il layer più importante nell'architettura *Transformer*? Qual è il suo scopo?\n",
        "\n",
        "  Il layer più importante nell'archittettura Transformer è il Multi-Head Attention layer. Esso utilizza delle rappresentazioni in vettori dei concetti sintattici (es. soggetto, verbo, ...) delle varie parole per trovare una similarità fra la parola input e la parola target. Per calcolare questa similarità utilizza un tipo di prodotto scalare regolarizzato chiamato *Scaled Dot-Product Attention*.\n",
        "\n",
        "7. Quando si utilizza il *sampled softmax*?\n",
        "\n",
        "  Il sampled softmax si utilizza in NMT quando si ha un vocabolario molto ampio e quindi calcolare la probabilità di ogni singola parola sarebbe troppo dispendioso. Per ovviare a questo problema, la probabilità si calcola solo sulla parola corretta e su un campione (un sample) di parole errate prese casualmente.\n",
        "\n",
        "10. Esegui il tutorial di TensorFlow sul NMT con Attention\n",
        "\n",
        "  Il tutorial si trova a questo [link](https://www.tensorflow.org/tutorials/text/nmt_with_attention).\n",
        "\n",
        "  TensorFlow ha realizzato anche un [tutorial simile per i Transformer](https://www.tensorflow.org/tutorials/text/transformer). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhAwkFqm40_"
      },
      "source": [
        "9. Addestra un modello Encoder-Decoder per convertire delle date da un formato all'altro (es. da\"April 22, 2019\" a \"2019-04-22\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmFW6cVkpy9Y"
      },
      "source": [
        "(Il codice è preso dalle soluzioni incluse nel libro)\n",
        "\n",
        "**Creazione del dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncNGrA3nm6c-"
      },
      "source": [
        "import numpy as np\n",
        "from datetime import date\n",
        "\n",
        "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
        "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "\n",
        "def random_dates(n_dates):\n",
        "    min_date = date(1000, 1, 1).toordinal()\n",
        "    max_date = date(9999, 12, 31).toordinal()\n",
        "\n",
        "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
        "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
        "\n",
        "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
        "    y = [dt.isoformat() for dt in dates]\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t32TeGc7pXcr",
        "outputId": "91a0148e-ee81-4ac4-f17a-04c805f6dcc2"
      },
      "source": [
        "x, y = random_dates(1)\n",
        "print(f\"Input sequence: {x}, output sequence: {y}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence: ['January 01, 6478'], output sequence: ['6478-01-01']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXH4NxoPrTOJ"
      },
      "source": [
        "* Tutti i possibili caratteri in input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o7yztnI0qmjP",
        "outputId": "56c5ec67-24e6-4452-e0b1-958b828d8af6"
      },
      "source": [
        "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
        "INPUT_CHARS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' ,0123456789ADFJMNOSabceghilmnoprstuvy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXUe6il8rXM4"
      },
      "source": [
        "* Tutti i possibili caratteri in output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_Awz9gKrXd1"
      },
      "source": [
        "OUTPUT_CHARS = \"0123456789-\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjpofKNLrkx3"
      },
      "source": [
        "* Funzione per convertire una stringa in una lista di ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWmHiap0rqwZ",
        "outputId": "40650620-18ac-4414-fed5-07fb65c53f40"
      },
      "source": [
        "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
        "    return [chars.index(c) for c in date_str]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15, 20, 29, 35, 20, 32, 37, 0, 2, 3, 1, 0, 8, 6, 9, 10]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnhffpI9sVBw",
        "outputId": "03d6be31-ce8a-45d4-b42d-7c226d633be4"
      },
      "source": [
        "date_str_to_ids(x[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15, 20, 29, 35, 20, 32, 37, 0, 2, 3, 1, 0, 8, 6, 9, 10]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8NUnksRruiM",
        "outputId": "8ffe3e7f-18c0-44ba-e933-a3fadcf73d87"
      },
      "source": [
        "date_str_to_ids(y[0], OUTPUT_CHARS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 4, 7, 8, 10, 0, 1, 10, 0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1ByQDgMsTft"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
        "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
        "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
        "    return (X + 1).to_tensor() # using 0 as the padding token ID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v3NTLZrskl2"
      },
      "source": [
        "def create_dataset(n_dates):\n",
        "    x, y = random_dates(n_dates)\n",
        "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X_train, Y_train = create_dataset(10000)\n",
        "X_valid, Y_valid = create_dataset(2000)\n",
        "X_test, Y_test = create_dataset(2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA8iOcB1tZyx"
      },
      "source": [
        "**Creazione del modello**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCPye2vyuWRe"
      },
      "source": [
        "L'encoder è formato da un Embedding layer seguito da un Long short-term memory layer. L'output dell'encoder, un vettore, viene passato al decoder, che è formato da un Long short-term memory layer e un Dense layer con attivazione softmax. L'output del decoder è un vettore contenente le probabilità per tutti i possibili caratteri di output (`OUTPUT_CHARS`).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPbNi-V_tZIQ"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "embedding_size = 32\n",
        "max_output_length = Y_train.shape[1]\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "encoder = keras.models.Sequential([\n",
        "    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1,\n",
        "                           output_dim=embedding_size,\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.LSTM(128)\n",
        "])\n",
        "\n",
        "decoder = keras.models.Sequential([\n",
        "    keras.layers.LSTM(128, return_sequences=True),\n",
        "    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    encoder,\n",
        "    keras.layers.RepeatVector(max_output_length),\n",
        "    decoder\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Nadam()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZuf5JcTtfqS"
      },
      "source": [
        "* Addestramento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHxgDlZgthCa",
        "outputId": "298a137e-13e9-4267-9c64-f982438b3813"
      },
      "source": [
        "history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "313/313 [==============================] - 18s 45ms/step - loss: 2.0799 - accuracy: 0.2643 - val_loss: 1.3714 - val_accuracy: 0.4938\n",
            "Epoch 2/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 1.2944 - accuracy: 0.5237 - val_loss: 1.0406 - val_accuracy: 0.6212\n",
            "Epoch 3/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 1.0312 - accuracy: 0.6311 - val_loss: 0.8331 - val_accuracy: 0.6978\n",
            "Epoch 4/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.8362 - accuracy: 0.6997 - val_loss: 0.6144 - val_accuracy: 0.7645\n",
            "Epoch 5/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.5807 - accuracy: 0.7782 - val_loss: 0.4772 - val_accuracy: 0.8145\n",
            "Epoch 6/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.4087 - accuracy: 0.8422 - val_loss: 0.3090 - val_accuracy: 0.8826\n",
            "Epoch 7/20\n",
            "313/313 [==============================] - 13s 41ms/step - loss: 0.3920 - accuracy: 0.8630 - val_loss: 0.3521 - val_accuracy: 0.8807\n",
            "Epoch 8/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.2851 - accuracy: 0.9070 - val_loss: 0.5364 - val_accuracy: 0.8286\n",
            "Epoch 9/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.2703 - accuracy: 0.9207 - val_loss: 0.1181 - val_accuracy: 0.9699\n",
            "Epoch 10/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.0935 - accuracy: 0.9806 - val_loss: 0.0651 - val_accuracy: 0.9872\n",
            "Epoch 11/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.0524 - accuracy: 0.9922 - val_loss: 0.0376 - val_accuracy: 0.9956\n",
            "Epoch 12/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.0298 - accuracy: 0.9972 - val_loss: 0.0222 - val_accuracy: 0.9983\n",
            "Epoch 13/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.0466 - accuracy: 0.9924 - val_loss: 0.0486 - val_accuracy: 0.9942\n",
            "Epoch 14/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.0293 - accuracy: 0.9980 - val_loss: 0.0157 - val_accuracy: 0.9988\n",
            "Epoch 15/20\n",
            "313/313 [==============================] - 13s 43ms/step - loss: 0.0119 - accuracy: 0.9998 - val_loss: 0.0101 - val_accuracy: 0.9995\n",
            "Epoch 16/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.0080 - accuracy: 0.9999 - val_loss: 0.0074 - val_accuracy: 0.9996\n",
            "Epoch 17/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 0.9998\n",
            "Epoch 18/20\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 0.9999\n",
            "Epoch 19/20\n",
            "313/313 [==============================] - 13s 43ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9999\n",
            "Epoch 20/20\n",
            "313/313 [==============================] - 14s 43ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-VG9IjnvPaY"
      },
      "source": [
        "* Per vedere le date in formato stringa occorre convertire gli ID dei caratteri"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOX7ZYgLvT3H"
      },
      "source": [
        "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
        "    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n",
        "            for sequence in ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMwa6dc5vqH1",
        "outputId": "364b114b-0fba-4780-87e4-b472afa1f211"
      },
      "source": [
        "X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])\n",
        "# ids = model.predict_classes(X_new) deprecato\n",
        "ids = np.argmax(model.predict(X_new), axis=-1)\n",
        "for date_str in ids_to_date_strs(ids):\n",
        "    print(date_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2009-09-17\n",
            "1789-07-14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGhxIdUtwaCH",
        "outputId": "9cad9938-ab08-4ee6-d8a6-2d44949ade3e"
      },
      "source": [
        "X_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"])\n",
        "ids = np.argmax(model.predict(X_new), axis=-1)\n",
        "for date_str in ids_to_date_strs(ids):\n",
        "    print(date_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-02\n",
            "1789-01-14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lw5N-C8vIeD"
      },
      "source": [
        "* Dopo 20 epoche di training, il modello raggiunge una precisione del 100%. Tuttavia, con input di lunghezza minori di 18 caratteri, non è affidabile. Per ovviare a questo problema, bisogna fornire in ingresso date lunghe 18 caratteri, aggiungendo un eventuale padding finale:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtkSJilswrcC"
      },
      "source": [
        "max_input_length = X_train.shape[1]\n",
        "\n",
        "def prepare_date_strs_padded(date_strs):\n",
        "    X = prepare_date_strs(date_strs)\n",
        "    if X.shape[1] < max_input_length:\n",
        "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
        "    return X\n",
        "\n",
        "def convert_date_strs(date_strs):\n",
        "    X = prepare_date_strs_padded(date_strs)\n",
        "    ids = np.argmax(model.predict(X), axis=-1)\n",
        "    return ids_to_date_strs(ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGgCl6Zfw0FA",
        "outputId": "12123acf-0a1f-4cfe-e8c7-9461c8f18e19"
      },
      "source": [
        "convert_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2020-05-02', '1789-07-14']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqvFgy_3fvnJ"
      },
      "source": [
        "10. Utilizzare uno dei più recenti modelli di NLP (BERT, GPT, ...) per generare del testo shakespeariano."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFVy6guWf-o5"
      },
      "source": [
        "Per implementare uno dei modelli più avanzati citati nel libro, si può fare uso della libreria open source *Transformers* di Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-kftYOziYX_",
        "outputId": "f9810ec1-e92c-49d6-f5d4-a62613e2c7b5"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 44.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 45.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOpq36YTijfr"
      },
      "source": [
        "In questo caso, viene implementato il modello GPT di OpenAI in una configurazione già addestrata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWdhGwjmf-Uy",
        "outputId": "ff0b0bf7-d66a-493a-dc72-f5968e2a15c6"
      },
      "source": [
        "from transformers import TFOpenAIGPTLMHeadModel\n",
        "\n",
        "model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFOpenAIGPTLMHeadModel.\n",
            "\n",
            "All the layers of TFOpenAIGPTLMHeadModel were initialized from the model checkpoint at openai-gpt.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6wHM-qliuNA"
      },
      "source": [
        "Come tokenizer, di default verrà utilizzato il `BasicTokenizer` del modello BERT. La documentazione riporta che questo tokenizer dovrebbe funzionare bene per la maggior parte degli usi, altrimenti sarà possibile sostituirlo con quelli utilizzati nel *paper* originale (SpaCy e ftfy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_DuDxPtjKFR",
        "outputId": "4576842c-df3f-4dc7-894e-b84509a9b4b5"
      },
      "source": [
        "from transformers import OpenAIGPTTokenizer\n",
        "\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpYonYCPjWz_"
      },
      "source": [
        "Test del tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCVk6jMljS6C",
        "outputId": "921013e3-775b-408f-b7ae-1c40f5dcea80"
      },
      "source": [
        "prompt_text = \"This royal throne of kings, this sceptred isle\"\n",
        "encoded_prompt = tokenizer.encode(prompt_text,\n",
        "                                  add_special_tokens=False,\n",
        "                                  return_tensors=\"tf\")\n",
        "encoded_prompt"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
              "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
              "        16187]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKj4CBQ8jhsP"
      },
      "source": [
        "Ora possiamo utilizzare il modello. L'idea è di partire da un testo in input, e da quello generare $n$ sequenze di $k$ caratteri. Il tuning degli iperparametri è spiegato in [questo articolo](https://huggingface.co/blog/how-to-generate) sul blog di Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDj6HXHwjYJo",
        "outputId": "d7d66a2e-9c84-4897-8522-263b5fd62c83"
      },
      "source": [
        "num_sequences = 3\n",
        "length = 50\n",
        "\n",
        "generated_sequences = model.generate(\n",
        "    input_ids=encoded_prompt, # sequenza di input\n",
        "    do_sample=True, # attivazione del sampling\n",
        "    max_length=length + len(encoded_prompt[0]), # lunghezza dell'output (length + input)\n",
        "    temperature=1.0, # la temperatura definisce quanto il modello è \"sensibile\" ai candidati con bassa proabilità\n",
        "    top_k=0, # disattivazione del top_k sampling\n",
        "    top_p=0.9, # nucleus sampling\n",
        "    repetition_penalty=1.0, # penalizzazione di parole già utilizzate\n",
        "    num_return_sequences=num_sequences, # numero di sequenze da generare\n",
        ")\n",
        "\n",
        "generated_sequences"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 60), dtype=int32, numpy=\n",
              "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
              "        16187,   544,   246,  2592,   488,  3128,  1082,   239,   616,\n",
              "          544,   895,   512,   635,   538,  1085,   246,  1082,   485,\n",
              "         2830,   551,   239,   616,   566,   635,   580,  3072,   267,\n",
              "          256, 40477,   256,   249,   993,   538,   587,   525,   267,\n",
              "          481,   618,   812,   848,   793,   669,   487,  7909,   239,\n",
              "          256, 40477,   256,   674,   512,  1259],\n",
              "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
              "        16187,   240,   645,   507,   641,   595,   562,   481,  6308,\n",
              "         4189,   877,   498,   481,  3573,   240,   636,   604,  1256,\n",
              "          725,  4837,   485,   481,  1657,  6903,   239,  5476,   507,\n",
              "          509,   525,   669,   481,   966, 14154,   485,  3013,   481,\n",
              "        15577,   240,   488,   481,  2565,   498,   481, 32661,   509,\n",
              "          885,   525,   655,   994,   580,   664],\n",
              "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
              "        16187,   509,  1233,   609,   246,   963,  7779,   615,   500,\n",
              "          984,   512,   635,   538,  2177,   670, 12593,  7174,   240,\n",
              "          595,   784,   500,   481,  1028,   498,   481,  9606,   239,\n",
              "          244, 40477,   244,   568,   240,   244,  7698,   603,   240,\n",
              "          244,   544,   538,   525,  2168,   485,  1315,   525,   249,\n",
              "          719,   485,   580,   618,   257,   244]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIgncsfUkXyv"
      },
      "source": [
        "L'output è in formato tokenizzato e va convertito al formato testuale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dalsM9Gaj3pw",
        "outputId": "99410773-9faf-48ae-bd63-8ed9d17624cb"
      },
      "source": [
        "for sequence in generated_sequences:\n",
        "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
        "    print(text)\n",
        "    print(\".\" * 80)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this royal throne of kings, this sceptred isle is a serious and powerful place. this is why you couldn't find a place to hide out. this one could be dangerous!'\n",
            "'i can't do that! the king will come here when he wishes.'\n",
            "'then you must\n",
            "................................................................................\n",
            "this royal throne of kings, this sceptred isle, if it were not for the fierce governance of the island, would have done more harm to the whole realm. thus it was that when the need arose to protect the succession, and the promise of the scepter was made that there should be no\n",
            "................................................................................\n",
            "this royal throne of kings, this sceptred isle was set up a very peaceful thing in which you couldn't worry about tax laws, not even in the age of the kings. \" \n",
            " \" but, \" garion said, \" isn't that supposed to mean that i'm to be king? \"\n",
            "................................................................................\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}